#!/usr/bin/env python
"""
inference.py

datasets/all_data_hours/ ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨ datasets/all_features/ ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦æ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚

ç”»åƒãƒ‡ãƒ¼ã‚¿: ç›´è¿‘4æ™‚é–“åˆ† (1, 4, 10, 256, 256)
ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿: ç›´è¿‘672æ™‚é–“åˆ† (1, 672, 128)

æ¨è«–çµæœã¯ ../data/pred_24.json ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚

å®Ÿè¡Œä¾‹:
$ python inference.py --params params/main/params.yaml --fold 1 --data_root ./datasets --cuda_device 0 --history 4 --trial_name 090 --mode test --resume_from_checkpoint checkpoints/main/216_stage1_best.pth
"""

import os
import sys
import json
import h5py
import torch
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from argparse import ArgumentParser, Namespace
from types import SimpleNamespace
import yaml
from models.main.models.ours import Ours

def simple_parse_params(dump=False):
    """
    Simplified parse_params function that only loads what we need for inference
    """
    class SimpleConfig:
        def __init__(self):
            self.cache_root = "ml/datasets/main/cache"  # Cache root path for main (not pretrain)
            self.data_root = "ml/datasets"  # Default data root
            self.fold = 1  # Default fold
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ãƒ¢ãƒƒã‚¯
            self.model = SimpleNamespace()
            self.model.selected = "Ours"
            self.model.models = SimpleNamespace()
            self.model.models.Ours = SimpleNamespace()
            self.model.models.Ours.architecture_params = SimpleNamespace()
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            self.model.models.Ours.architecture_params.in_chans = 10
            self.model.models.Ours.architecture_params.embed_dim = 768
            self.model.models.Ours.architecture_params.depth = 12
            self.model.models.Ours.architecture_params.num_heads = 12
            self.model.models.Ours.architecture_params.mlp_ratio = 4.0
            self.model.models.Ours.architecture_params.drop_rate = 0.1
            self.model.models.Ours.architecture_params.attn_drop_rate = 0.1
    
    config = SimpleConfig()
    return config, {}

def load_checkpoint(model, checkpoint_path):
    state = torch.load(checkpoint_path, map_location="cpu")
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒ "model_state_dict" ã‚’æŒã£ã¦ã„ã‚‹å ´åˆ
    if "model_state_dict" in state:
        state_dict = state["model_state_dict"]
    else:
        state_dict = state

    # "total_ops" ã‚„ "total_params" ã‚’å«ã‚€ã‚­ãƒ¼ã‚’é™¤å¤–
    filtered_state_dict = {k: v for k, v in state_dict.items() if "total_ops" not in k and "total_params" not in k}

    model.load_state_dict(filtered_state_dict)
    return model

def namespace_to_dict(obj):
    """å†å¸°çš„ã« Namespace ã‚„ list ã‚’è¾æ›¸ã«å¤‰æ›ã™ã‚‹"""
    if isinstance(obj, (Namespace, SimpleNamespace)):
        return {k: namespace_to_dict(v) for k, v in obj.__dict__.items()}
    elif isinstance(obj, dict):
        return {k: namespace_to_dict(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [namespace_to_dict(v) for v in obj]
    else:
        return obj

def load_feature_data(feature_path):
    """ç‰¹å¾´é‡H5ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with h5py.File(feature_path, "r") as f:
            features = f["features"][:]  # shape: (128,)
            # NaNå€¤ã‚„ç„¡é™å¤§å€¤ã‚’0ã«ç½®æ›
            features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
            return features
    except Exception as e:
        print(f"Warning: ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {feature_path}: {e}")
        return None

def get_features_for_timerange(feature_dir, target_time, hours=672):
    """æŒ‡å®šã—ãŸæ™‚åˆ»ã‹ã‚‰éå»hoursæ™‚é–“åˆ†ã®ç‰¹å¾´é‡ã‚’å–å¾—"""
    features_list = []
    valid_features_count = 0
    
    for i in range(hours):
        # target_timeã‹ã‚‰ i æ™‚é–“å‰ã®æ™‚åˆ»ã‚’è¨ˆç®—
        current_time = target_time - timedelta(hours=i)
        feature_filename = current_time.strftime("%Y%m%d_%H0000.h5")
        feature_path = os.path.join(feature_dir, feature_filename)
        
        if os.path.exists(feature_path):
            features = load_feature_data(feature_path)
            if features is not None:
                # NaNã‚„ç„¡é™å¤§ã‚’0ã«ç½®æ›
                features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
                features_list.append(features)
                valid_features_count += 1
            else:
                # èª­ã¿è¾¼ã¿å¤±æ•—ã®å ´åˆã¯0åŸ‹ã‚
                features_list.append(np.zeros(128, dtype=np.float32))
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯0åŸ‹ã‚
            features_list.append(np.zeros(128, dtype=np.float32))
    
    # æ™‚ç³»åˆ—é †ã«ä¸¦ã¹ç›´ã™ï¼ˆæœ€å¤â†’æœ€æ–°ï¼‰
    features_list.reverse()
    
    # ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ (672, 128) ã«ã™ã‚‹
    features_array = np.stack(features_list, axis=0)
    
    # æœ€çµ‚çš„ã«ã‚‚ä¸€åº¦NaNå‡¦ç†ã‚’å®Ÿè¡Œ
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    return features_array, valid_features_count

def main():
    parser = ArgumentParser()
    parser.add_argument("--params", required=True, help="YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--fold", type=int, required=True, help="Foldç•ªå·")
    parser.add_argument("--data_root", default="./datasets", help="datasetsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆ")
    parser.add_argument("--cuda_device", type=int, default=0)
    parser.add_argument("--history", type=int, default=4, help="ä½¿ç”¨ã™ã‚‹å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆä¾‹: 4ï¼‰")
    parser.add_argument("--trial_name", default="idxxxx")
    parser.add_argument("--mode", default="test")
    parser.add_argument("--resume_from_checkpoint", required=True, help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹")
    parser.add_argument("--debug", action="store_true", help="ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã›ãšã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã«å‡ºåŠ›ã—ã¾ã™")
    parser.add_argument("--datetime", help="å‡¦ç†å¯¾è±¡ã®æ—¥æ™‚ (YYYYMMDD_HHMMSSå½¢å¼) - ç‰¹å®šã®æ™‚åˆ»ã®ã¿å‡¦ç†")
    args = parser.parse_args()

    # æ—¢å­˜ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    args_config, yaml_config = simple_parse_params()
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã®å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¸Šæ›¸ã
    args_config.fold = args.fold
    args_config.data_root = args.data_root
    args_config.cuda_device = args.cuda_device
    args_config.history = args.history
    args_config.trial_name = args.trial_name
    args_config.mode = args.mode
    args_config.resume_from_checkpoint = args.resume_from_checkpoint
    # æ¨è«–ã¯CPUä¸Šã§å®Ÿæ–½ã™ã‚‹
    args_config.device = "cpu"
    # h5ãƒ•ã‚¡ã‚¤ãƒ«ã®æ ¼ç´å…ˆã¯ all_data_hours ä»¥ä¸‹
    args_config.data_path = os.path.join(args_config.data_root, "all_data_hours")
    # ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ ¼ç´å…ˆ
    feature_dir = os.path.join(args_config.data_root, "all_features")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµ±è¨ˆæƒ…å ±ï¼ˆmeans.npy, stds.npyï¼‰ã‚’èª­ã¿è¾¼ã‚€
    fold_dir = os.path.join(args_config.cache_root, f"fold{args_config.fold}")
    train_cache_dir = os.path.join(fold_dir, "train")
    means_path = os.path.join(train_cache_dir, "means.npy")
    stds_path = os.path.join(train_cache_dir, "stds.npy")
    if not os.path.exists(means_path) or not os.path.exists(stds_path):
        print("Error: Cached statistics (means.npy or stds.npy) not found.")
        sys.exit(1)
    means = np.load(means_path)   # shape: (10,)
    stds = np.load(stds_path)       # shape: (10,)

    # å…¨ã¦ã® h5 ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ã—ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
    all_files = [f for f in os.listdir(args_config.data_path) if f.endswith(".h5")]
    file_dict = {}
    for f in all_files:
        # ãƒ•ã‚¡ã‚¤ãƒ«åä¾‹: "20250401_000000.h5"
        try:
            ts_str = os.path.splitext(f)[0]
            ts = datetime.strptime(ts_str, "%Y%m%d_%H%M%S")
            file_dict[ts] = os.path.join(args_config.data_path, f)
        except Exception as e:
            print(f"Warning: ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—: {f} ({e})")
            continue

    if not file_dict:
        print("Error: æœ‰åŠ¹ãª h5 ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(1)

    sorted_times = sorted(file_dict.keys())

    # æ¨è«–å¯èƒ½ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ã‚’æŠ½å‡º
    if args.datetime:
        # ç‰¹å®šã®æ—¥æ™‚ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
        try:
            target_time = datetime.strptime(args.datetime, "%Y%m%d_%H%M%S")
            # æŒ‡å®šã•ã‚ŒãŸæ™‚åˆ»ã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ1æ™‚é–“é–“éš”ï¼‰
            required = [target_time - timedelta(hours=i) for i in range(0, args_config.history)]
            # æ¬ æã¯0åŸ‹ã‚ã§å¯¾å¿œã™ã‚‹ã®ã§ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°å‡¦ç†å¯èƒ½
            if target_time in file_dict:
                valid_targets = [target_time]
            else:
                print(f"Error: æŒ‡å®šã•ã‚ŒãŸæ™‚åˆ» {args.datetime} ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:")
                print(f"å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«: {target_time.strftime('%Y%m%d_%H%M%S')}.h5")
                sys.exit(1)
        except ValueError:
            print(f"Error: æ—¥æ™‚ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {args.datetime}")
            print("æ­£ã—ã„å½¢å¼: YYYYMMDD_HHMMSS (ä¾‹: 20250615_180000)")
            sys.exit(1)
    else:
        # å…¨ã¦ã®åˆ©ç”¨å¯èƒ½ãªæ™‚åˆ»ã§æ¨è«–
        # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ» T ã«å¯¾ã—ã¦ã€T, T-1h, T-2h, T-3h ãŒå­˜åœ¨ã™ã‚Œã°å¯¾è±¡ã¨ã™ã‚‹
        valid_targets = []
        for t in sorted_times:
            required = [t - timedelta(hours=i) for i in range(0, args_config.history)]
            if all(rt in file_dict for rt in required):
                valid_targets.append(t)

    if not valid_targets:
        print("Error: æ¨è«–å¯èƒ½ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(1)

    print("æ¨è«–å¯èƒ½ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ï¼ˆæœ€æ–°æ™‚åˆ»ï¼‰ä¸€è¦§:")
    for vt in valid_targets:
        print(vt.strftime("%Y%m%d_%H%M%S"))

    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ï¼ˆconfig ã®è¨­å®šã‹ã‚‰ architecture_params ã‚’ä½¿ç”¨ï¼‰
    # SimpleNamespaceã‹ã‚‰ç›´æ¥è¾æ›¸ã‚’ä½œæˆ
    params_dict = {
        'D': 64,  # Feature dimension
        'drop_path_rate': 0.6,
        'layer_scale_init_value': 1.0e-6,
        'L': 128,  # Sequence length
        'L_SSE': 3,  # Number of SolarSpatialEncoder layers
        'L_LT': 1,  # Number of LongRangeTemporalSSM layers
        'L_mixing': 2,  # Number of MixingSSM layers
        'dropout_rates': {
            'sse': 0.6,
            'dwcm': 0.6,
            'stssm': 0.6,
            'ltssm': 0.6,
            'mixing_ssm': 0.6,
            'head': 0.6
        }
    }
    model = Ours(**params_dict)
    model.to("cpu")
    model.eval()
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
    load_checkpoint(model, args_config.resume_from_checkpoint)

    predictions = {}
    # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ã«ã¤ã„ã¦æ¨è«–ã‚’å®Ÿæ–½
    for target in valid_targets:
        print(f"\nå‡¦ç†ä¸­: {target.strftime('%Y%m%d_%H%M%S')}")
        
        # === ç”»åƒãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ (ç›´è¿‘4æ™‚é–“åˆ†) ===
        # required_times ã¯ [T-3h, T-2h, T-1h, T] ã®æ˜‡é †ã®ãƒªã‚¹ãƒˆ
        required_times = [target - timedelta(hours=args_config.history - 1 - i) for i in range(args_config.history)]
        X_list = []
        valid_images_count = 0
        
        for rt in required_times:
            file_path = file_dict.get(rt)
            if file_path is None:
                print(f"Warning: {rt.strftime('%Y%m%d_%H%M%S')} ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚0åŸ‹ã‚ã—ã¾ã™ã€‚")
                # æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ã¯0åŸ‹ã‚ï¼ˆ10, 256, 256ï¼‰
                X_data = np.zeros((10, 256, 256), dtype=np.float32)
                X_list.append(X_data)
            else:
                with h5py.File(file_path, "r") as f:
                    X_data = f["X"][:]  # shape: (10,256,256) ã‚’æƒ³å®š
                    X_list.append(X_data)
                    valid_images_count += 1
        
        # === ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ (ç›´è¿‘672æ™‚é–“åˆ†) ===
        features_array, valid_features_count = get_features_for_timerange(feature_dir, target, hours=672)
        
        # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’è¡¨ç¤º
        image_missing_rate = (args_config.history - valid_images_count) / args_config.history * 100
        feature_missing_rate = (672 - valid_features_count) / 672 * 100
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿çµ±è¨ˆ:")
        print(f"  ç”»åƒãƒ‡ãƒ¼ã‚¿: {valid_images_count}/{args_config.history} å€‹ (æ¬ æç‡: {image_missing_rate:.1f}%)")
        print(f"  ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿: {valid_features_count}/672 å€‹ (æ¬ æç‡: {feature_missing_rate:.1f}%)")
        
        # 0åŸ‹ã‚ã§å¯¾å¿œã™ã‚‹ã®ã§ã€å¸¸ã«historyåˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒæƒã†
        if len(X_list) != args_config.history:
            print(f"Error: äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_list)} (æœŸå¾…å€¤: {args_config.history})")
            continue
            
        # ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦å½¢çŠ¶ (history, 10, 256,256) ã«ã™ã‚‹
        X_array = np.stack(X_list, axis=0)
        
        # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª ===
        print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        print(f"  ç”Ÿç”»åƒãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ: min={np.min(X_array):.4f}, max={np.max(X_array):.4f}, mean={np.mean(X_array):.4f}")
        print(f"  ç”Ÿç”»åƒãƒ‡ãƒ¼ã‚¿NaNæ•°: {np.sum(np.isnan(X_array))}")
        print(f"  ç”Ÿç”»åƒãƒ‡ãƒ¼ã‚¿infæ•°: {np.sum(np.isinf(X_array))}")
        
        X_array = np.nan_to_num(X_array, 0)
        
        # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±: æ­£è¦åŒ–çµ±è¨ˆã®ç¢ºèª ===
        print(f"  æ­£è¦åŒ–çµ±è¨ˆ means: min={np.min(means):.4f}, max={np.max(means):.4f}")
        print(f"  æ­£è¦åŒ–çµ±è¨ˆ stds: min={np.min(stds):.4f}, max={np.max(stds):.4f}")
        print(f"  æ­£è¦åŒ–çµ±è¨ˆã«NaN: means={np.sum(np.isnan(means))}, stds={np.sum(np.isnan(stds))}")
        
        # æ­£è¦åŒ–ï¼šå„ãƒãƒ£ãƒãƒ«æ¯ã« (x - mean) / (std + 1e-8)
        X_norm = (X_array - means[None, :, None, None]) / (stds[None, :, None, None] + 1e-8)
        
        # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±: æ­£è¦åŒ–å¾Œãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª ===
        print(f"  æ­£è¦åŒ–å¾Œç”»åƒãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ: min={np.min(X_norm):.4f}, max={np.max(X_norm):.4f}, mean={np.mean(X_norm):.4f}")
        print(f"  æ­£è¦åŒ–å¾Œç”»åƒãƒ‡ãƒ¼ã‚¿NaNæ•°: {np.sum(np.isnan(X_norm))}")
        print(f"  æ­£è¦åŒ–å¾Œç”»åƒãƒ‡ãƒ¼ã‚¿infæ•°: {np.sum(np.isinf(X_norm))}")
        
        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã€ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ  â†’ (1, history, 10,256,256)
        sample_input1 = torch.from_numpy(X_norm).float().unsqueeze(0)
        
        # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª ===
        print(f"  ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ: min={np.min(features_array):.4f}, max={np.max(features_array):.4f}, mean={np.mean(features_array):.4f}")
        print(f"  ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿NaNæ•°: {np.sum(np.isnan(features_array))}")
        print(f"  ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿infæ•°: {np.sum(np.isinf(features_array))}")
        
        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã€ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ  â†’ (1, 672, 128)
        sample_input2 = torch.from_numpy(features_array).float().unsqueeze(0)
        
        print(f"ç”»åƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {sample_input1.shape}")
        print(f"ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {sample_input2.shape}")
        
        # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ãƒ¢ãƒ‡ãƒ«æ¨è«–å‰ã®å…¥åŠ›ç¢ºèª ===
        print(f"  å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«1 NaNæ•°: {torch.sum(torch.isnan(sample_input1)).item()}")
        print(f"  å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«1 infæ•°: {torch.sum(torch.isinf(sample_input1)).item()}")
        print(f"  å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«2 NaNæ•°: {torch.sum(torch.isnan(sample_input2)).item()}")
        print(f"  å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«2 infæ•°: {torch.sum(torch.isinf(sample_input2)).item()}")
        
        with torch.no_grad():
            # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®ç¢ºèª ===
            has_nan_weights = False
            for name, param in model.named_parameters():
                if torch.sum(torch.isnan(param)).item() > 0:
                    print(f"  âš ï¸ ãƒ¢ãƒ‡ãƒ«é‡ã¿ '{name}' ã«NaNãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
                    has_nan_weights = True
            if not has_nan_weights:
                print(f"  âœ… ãƒ¢ãƒ‡ãƒ«é‡ã¿ã«NaNã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            
            logits, _ = model(sample_input1, sample_input2)
            
            # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±: logitsã®ç¢ºèª ===
            print(f"  logitsçµ±è¨ˆ: min={torch.min(logits).item():.4f}, max={torch.max(logits).item():.4f}, mean={torch.mean(logits).item():.4f}")
            print(f"  logits NaNæ•°: {torch.sum(torch.isnan(logits)).item()}")
            print(f"  logits infæ•°: {torch.sum(torch.isinf(logits)).item()}")
            print(f"  logitså€¤: {logits[0].cpu().numpy()}")
            
            probs = torch.softmax(logits, dim=1)
            
            # === ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ç¢ºç‡ã®ç¢ºèª ===
            print(f"  probsçµ±è¨ˆ: min={torch.min(probs).item():.4f}, max={torch.max(probs).item():.4f}, sum={torch.sum(probs).item():.4f}")
            print(f"  probs NaNæ•°: {torch.sum(torch.isnan(probs)).item()}")
            print(f"  probs infæ•°: {torch.sum(torch.isinf(probs)).item()}")
            
            probs_list = probs[0].cpu().numpy().tolist()
        
        # ã‚­ãƒ¼ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ï¼ˆæœ€æ–°ï¼‰ã® "YYYYMMDDHH" å½¢å¼
        key = target.strftime("%Y%m%d%H")
        predictions[key] = [round(p, 6) for p in probs_list]
        print(f"äºˆæ¸¬çµæœ: {probs_list}")

    if args.debug:
        print("Prediction results (debug mode):")
        for ts, probs in predictions.items():
            print(f"{ts}: {probs}")
    else:
        # çµæœã‚’ "../data/pred_24.json" ã«ä¿å­˜
        out_dir = os.path.join("..", "data")
        os.makedirs(out_dir, exist_ok=True)
        out_path = os.path.join(out_dir, "pred_24.json")
        with open(out_path, "w") as f:
            json.dump(predictions, f, indent=2)

        print(f"\nâœ… Saved predictions for {len(predictions)} timestamps to {out_path}")
        print("Prediction results:")
        for ts, probs in predictions.items():
            print(f"{ts}: {probs}")

if __name__ == "__main__":
    main()