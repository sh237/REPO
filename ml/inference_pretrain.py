#!/usr/bin/env python
"""
inference_pretrain.py

æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ãƒ»æ—¥æ™‚ã® h5 ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‹ã‚‰ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ä¸­é–“ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€ä¿å­˜ã—ã¾ã™ã€‚
å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ» T ã® h5 ãƒ•ã‚¡ã‚¤ãƒ« (ä¾‹: YYYYMMDD_HHMMSS.h5) ã‚’å…¥åŠ›ã¨ã—ã€
ãã®ç‰¹å¾´é‡ã‚’ ml/datasets/all_features/YYYYMMDD_HHMMSS.h5 ã«ä¿å­˜ã—ã¾ã™ã€‚

å®Ÿè¡Œä¾‹:
$ python inference_pretrain.py --params params/main/params.yaml --datetime 20250504_080000 --fold 1 --pretrain_checkpoint checkpoints/pretrain/ours.pth
$ python inference_pretrain.py --params params/main/params.yaml --date 20250504 --fold 1 --pretrain_checkpoint checkpoints/pretrain/ours.pth
"""

import os
import sys
import json
import h5py
import torch
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from argparse import ArgumentParser, Namespace
import yaml
# For pretrain models, import from the pretrain model definitions
from models.pretrain.mae import vit_for_FT128db, mae_vit_base_patch16_dec512d8b # Add other models if needed

def simple_parse_params(params_path, dump=False):
    """
    Simplified parse_params function that only loads what we need for inference_pretrain
    """
    class SimpleConfig:
        def __init__(self):
            self.cache_root = "datasets/pretrain/cache"  # Correct cache root path
            self.data_root = "./datasets"  # Default data root
            self.fold = 1  # Default fold
    
    config = SimpleConfig()
    
    # Try to load YAML file if it exists
    if params_path and os.path.exists(params_path):
        try:
            with open(params_path, 'r') as f:
                yaml_config = yaml.safe_load(f)
            
            # Update config with values from YAML if they exist
            if 'cache_root' in yaml_config:
                config.cache_root = yaml_config['cache_root']
            if 'data_root' in yaml_config:
                config.data_root = yaml_config['data_root']
            
            return config, yaml_config
        except:
            print(f"âš ï¸ Warning: Could not load YAML config from {params_path}, using defaults")
    
    return config, {}

def load_pretrain_checkpoint(model, checkpoint_path, map_location="cpu"):
    """äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    if not os.path.exists(checkpoint_path):
        print(f"âŒ Error: Pretrain checkpoint file not found at {checkpoint_path}")
        sys.exit(1)
    state = torch.load(checkpoint_path, map_location=map_location)
    
    model_state_dict = None
    if "model" in state:
        model_state_dict = state["model"]
    elif "state_dict" in state:
        model_state_dict = state["state_dict"]
    elif "model_state_dict" in state: # From main inference.py style
         model_state_dict = state["model_state_dict"]
    else:
        model_state_dict = state # Assume the checkpoint is the state_dict itself

    if model_state_dict is None:
        print(f"âŒ Error: Could not find model state_dict in checkpoint {checkpoint_path}")
        sys.exit(1)

    # Handle DataParallel/DistributedDataParallel prefix
    new_state_dict = {}
    for k, v in model_state_dict.items():
        if k.startswith('module.'):
            new_state_dict[k[7:]] = v  # remove `module.`
        else:
            new_state_dict[k] = v
    
    try:
        model.load_state_dict(new_state_dict)
        print(f"âœ… Pretrain model loaded from {checkpoint_path}")
    except RuntimeError as e:
        print(f"âŒ Error loading state_dict for pretrain model: {e}")
        print("This might be due to a mismatch in model architecture and checkpoint.")
        sys.exit(1)
    return model

def get_pretrain_model(model_arch, device):
    """æŒ‡å®šã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹"""
    # Define default parameters for pretrain models here
    # These should match the architecture used for 'ours.pth'
    # For vit_for_FT128db (example):
    model_params = {
        "in_chans": 10,
        "pyramid": True, # Example: ensure these match 'ours.pth'
        "sunspot": True, # Example: ensure these match 'ours.pth'
        # mask_ratio related params are not needed for feature extraction via forward_features
    }
    if model_arch == "vit_for_FT128db":
        model = vit_for_FT128db(**model_params)
    elif model_arch == "mae_vit_base_patch16_dec512d8b":
        # MAE model might need different default params
        model = mae_vit_base_patch16_dec512d8b(in_chans=10) # Simpler example
    else:
        print(f"âŒ Error: Unknown pretrain model architecture: {model_arch}")
        sys.exit(1)
    return model.to(device)


def main():
    parser = ArgumentParser()
    parser.add_argument("--params", required=True, help="YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ä¸»ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹ç­‰ã§ä½¿ç”¨)")
    parser.add_argument("--date", help="å‡¦ç†å¯¾è±¡ã®æ—¥ä»˜ (YYYYMMDDå½¢å¼) - ãã®æ—¥ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†")
    parser.add_argument("--datetime", help="å‡¦ç†å¯¾è±¡ã®æ—¥æ™‚ (YYYYMMDD_HHMMSSå½¢å¼) - ç‰¹å®šã®æ™‚åˆ»ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†")
    parser.add_argument("--fold", type=int, required=True, help="Foldç•ªå· (means/stdsã®èª­ã¿è¾¼ã¿ã«å¿…è¦)")
    parser.add_argument("--data_root", default="./datasets", help="datasetsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆ")
    parser.add_argument("--pretrain_checkpoint", default="checkpoints/pretrain/ours.pth", help="äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹")
    parser.add_argument("--model_arch", default="vit_for_FT128db", help="äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (ä¾‹: vit_for_FT128db, mae_vit_base_patch16_dec512d8b)")
    parser.add_argument("--output_feature_dir", default="datasets/all_features", help="æŠ½å‡ºã—ãŸç‰¹å¾´é‡ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--cuda_device", type=int, default=0, help="ä½¿ç”¨ã™ã‚‹CUDAãƒ‡ãƒã‚¤ã‚¹ç•ªå· (CPUã®å ´åˆã¯-1)")
    parser.add_argument("--debug", action="store_true", help="ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: å°‘æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†ã—ã€è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›")
    args = parser.parse_args()

    # --date ã¨ --datetime ã®æ’ä»–ãƒã‚§ãƒƒã‚¯
    if not args.date and not args.datetime:
        print("âŒ Error: Either --date (YYYYMMDD) or --datetime (YYYYMMDD_HHMMSS) must be specified.")
        sys.exit(1)
    if args.date and args.datetime:
        print("âŒ Error: Cannot specify both --date and --datetime. Use one or the other.")
        sys.exit(1)

    # parse_paramsã¯ä¸»ã«args_config.cache_rootç­‰ã®è¨­å®šã®ãŸã‚ã«å‘¼ã³å‡ºã™
    args_config, _ = simple_parse_params(args.params, dump=False) # yaml_configã¯ä¸è¦
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§args_configã®é–¢é€£å€¤ã‚’ä¸Šæ›¸ã
    args_config.data_root = args.data_root
    args_config.fold = args.fold
    
    if args.cuda_device >= 0 and torch.cuda.is_available():
        device = torch.device(f"cuda:{args.cuda_device}")
    else:
        device = torch.device("cpu")
    print(f"â„¹ï¸ Using device: {device}")

    # h5ãƒ•ã‚¡ã‚¤ãƒ«ã®æ ¼ç´å…ˆ (å…¥åŠ›å…ƒ)
    input_h5_dir = os.path.join(args_config.data_root, "all_data_hours")
    if not os.path.isdir(input_h5_dir):
        print(f"âŒ Error: Input H5 directory not found: {input_h5_dir}")
        sys.exit(1)

    # ç‰¹å¾´é‡ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(args.output_feature_dir, exist_ok=True)
    print(f"â„¹ï¸ Features will be saved to: {args.output_feature_dir}")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµ±è¨ˆæƒ…å ±ï¼ˆmeans.npy, stds.npyï¼‰ã‚’èª­ã¿è¾¼ã‚€
    # `fold` ã¯ args_config ã§ã¯ãªã args ã‹ã‚‰ç›´æ¥ä½¿ç”¨
    fold_dir = os.path.join(args_config.cache_root, f"fold{args.fold}")
    train_cache_dir = os.path.join(fold_dir, "train")
    means_path = os.path.join(train_cache_dir, "means.npy")
    stds_path = os.path.join(train_cache_dir, "stds.npy")

    if not os.path.exists(means_path) or not os.path.exists(stds_path):
        print(f"âŒ Error: Cached statistics (means.npy or stds.npy) not found in {train_cache_dir}")
        print("Please ensure pretraining/caching has been run for this fold.")
        sys.exit(1)
    means = np.load(means_path)   # shape: (C,) e.g. (10,)
    stds = np.load(stds_path)       # shape: (C,) e.g. (10,)
    print(f"âœ… Loaded means (shape: {means.shape}) and stds (shape: {stds.shape}) for fold {args.fold}")

    # å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    target_files = []
    if args.datetime:
        # ç‰¹å®šã®æ—¥æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†
        target_filename = f"{args.datetime}.h5"
        target_filepath = os.path.join(input_h5_dir, target_filename)
        if os.path.exists(target_filepath):
            target_files.append(target_filepath)
        else:
            print(f"âŒ Error: Specified datetime file not found: {target_filepath}")
            sys.exit(1)
    else:
        # æŒ‡å®šæ—¥ä»˜ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        for f_name in os.listdir(input_h5_dir):
            if f_name.startswith(args.date) and f_name.endswith(".h5"):
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åä¾‹: "20230101_000000.h5"
                    # æ—¥ä»˜ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    ts_str = os.path.splitext(f_name)[0]
                    datetime.strptime(ts_str, "%Y%m%d_%H%M%S") 
                    target_files.append(os.path.join(input_h5_dir, f_name))
                except ValueError:
                    print(f"âš ï¸ Warning: Could not parse timestamp from filename {f_name}. Skipping.")
    
    if not target_files:
        search_target = args.datetime if args.datetime else args.date
        print(f"â„¹ï¸ No H5 files found for {search_target} in {input_h5_dir}")
        sys.exit(0)
    
    target_files.sort() # æ™‚ç³»åˆ—é †ã«å‡¦ç†ã™ã‚‹ãŸã‚ã«ã‚½ãƒ¼ãƒˆ
    print(f"â„¹ï¸ Found {len(target_files)} files to process.")
    if args.debug:
        target_files = target_files[:3] # Debug: process only a few files
        print(f"ğŸ› Debug mode: processing first {len(target_files)} files.")

    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰
    pretrain_model = get_pretrain_model(args.model_arch, device)
    pretrain_model = load_pretrain_checkpoint(pretrain_model, args.pretrain_checkpoint, map_location=device)
    pretrain_model.eval()

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦ç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
    for h5_file_path in target_files:
        file_basename = os.path.basename(h5_file_path)
        output_feature_filename = file_basename  # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«åã§ä¿å­˜ (YYYYMMDD_HHMMSS.h5)
        output_feature_path = os.path.join(args.output_feature_dir, output_feature_filename)

        if os.path.exists(output_feature_path) and not args.debug: # Avoid re-computing if not in debug
            print(f"â­ï¸ Feature file already exists: {output_feature_path}. Skipping.")
            continue
        
        try:
            with h5py.File(h5_file_path, "r") as f:
                X_data = f["X"][:]  # Expected shape: (C, H, W), e.g., (10, 256, 256)
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚‚èª­ã¿è¾¼ã¿ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
                # timestampãŒã‚¹ã‚«ãƒ©ãƒ¼å€¤ã®å ´åˆã¯[:]ã§ã¯ãªã[()]ã‚’ä½¿ç”¨
                if "timestamp" in f:
                    timestamp_data = f["timestamp"]
                    if timestamp_data.shape == ():  # ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã®å ´åˆ
                        timestamp = timestamp_data[()].decode('utf-8') if isinstance(timestamp_data[()], bytes) else str(timestamp_data[()])
                    else:  # é…åˆ—ã®å ´åˆ
                        timestamp = timestamp_data[:].decode('utf-8') if isinstance(timestamp_data[:], bytes) else str(timestamp_data[:])
                else:
                    timestamp = os.path.splitext(file_basename)[0]
        except Exception as e:
            print(f"âŒ Error reading H5 file {h5_file_path}: {e}")
            continue

        if X_data.ndim != 3 or X_data.shape[0] != means.shape[0]: # Basic sanity check
            print(f"âŒ Error: Data in {h5_file_path} has unexpected shape {X_data.shape}. Expected ({means.shape[0]}, H, W). Skipping.")
            continue
            
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°æƒ…å ±ã‚’å‡ºåŠ›
        print(f"ğŸ“Š Input data info for {file_basename}:")
        print(f"  - Input shape: {X_data.shape}")
        print(f"  - Input data type: {X_data.dtype}")
        print(f"  - Input min: {X_data.min():.6f}")
        print(f"  - Input max: {X_data.max():.6f}")
        print(f"  - Input mean: {X_data.mean():.6f}")
        print(f"  - Input std: {X_data.std():.6f}")
        
        # æ­£è¦åŒ–: (C, H, W) -> (C, H, W)
        # means/stds are (C,), need to reshape for broadcasting: (C, 1, 1)
        X_norm = (X_data - means[:, np.newaxis, np.newaxis]) / (stds[:, np.newaxis, np.newaxis] + 1e-8)
        
        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã—ã€ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ  -> (1, C, H, W)
        input_tensor = torch.from_numpy(X_norm).float().unsqueeze(0).to(device)
        
        with torch.no_grad():
            try:
                # Use forward_encoder_pyramid like in extract_mae_features
                batch_features, _, _ = pretrain_model.forward_encoder_pyramid(input_tensor, mask_ratio=0.0)
                # Extract mean features excluding CLS token (skip [:, 1:, :])
                mean_features = batch_features[:, 1:, :].mean(dim=1)
                
                # Convert to numpy - features should now be (1, embed_dim) -> (embed_dim,)
                features_np = mean_features.squeeze(0).cpu().numpy()
                
                # ç‰¹å¾´é‡ã®è©³ç´°æƒ…å ±ã‚’å‡ºåŠ›
                print(f"ğŸ” Features info for {file_basename}:")
                print(f"  - Shape: {features_np.shape}")
                print(f"  - Data type: {features_np.dtype}")
                print(f"  - Min value: {features_np.min():.6f}")
                print(f"  - Max value: {features_np.max():.6f}")
                print(f"  - Mean: {features_np.mean():.6f}")
                print(f"  - Std: {features_np.std():.6f}")
                print(f"  - Feature dimension: {features_np.shape[0] if len(features_np.shape) == 1 else features_np.shape}")
                
            except Exception as e:
                print(f"âŒ Error during model inference for {h5_file_path}: {e}")
                continue
        
        try:
            # H5ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ç‰¹å¾´é‡ã‚’ä¿å­˜
            with h5py.File(output_feature_path, 'w') as f:
                f.create_dataset("features", data=features_np)
                f.create_dataset("timestamp", data=timestamp.encode('utf-8'))
                # å…ƒã®å½¢çŠ¶æƒ…å ±ã‚‚ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                f.attrs["original_shape"] = X_data.shape
                f.attrs["features_shape"] = features_np.shape
            print(f"âœ… Saved features for {file_basename} to {output_feature_path} (shape: {features_np.shape})")
        except Exception as e:
            print(f"âŒ Error saving features to {output_feature_path}: {e}")

    print("ğŸ Feature extraction complete.")

if __name__ == "__main__":
    main() 
    